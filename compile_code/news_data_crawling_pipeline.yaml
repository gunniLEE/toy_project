apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: news-data-crawling-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.9, pipelines.kubeflow.org/pipeline_compilation_time: '2022-08-15T19:51:00.346689',
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"name": "datetime", "type":
      "String"}], "name": "news_data_crawling_pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.9}
spec:
  entrypoint: news-data-crawling-pipeline
  templates:
  - name: crawling-data
    container:
      args: [--datetime, '{{inputs.parameters.datetime}}', --data, /tmp/outputs/data/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'beautifulsoup4' 'requests' 'fake_useragent' 'pandas' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'beautifulsoup4'
        'requests' 'fake_useragent' 'pandas' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef crawling_data(\n    datetime,\n    data_path,\n):  \n    import os\n\
        \    import sys\n    sys.path.append('/module')\n    from data_crawler import\
        \ NaverNewsCrawler\n\n    cralwer = NaverNewsCrawler(date=datetime)\n    data\
        \ = cralwer.get_news_data()\n    cralwer.create_news_data(data_path , data)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Crawling data',\
        \ description='')\n_parser.add_argument(\"--datetime\", dest=\"datetime\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --data\", dest=\"data_path\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = crawling_data(**_parsed_args)\n"
      image: python:3.7
      volumeMounts:
      - mountPath: /module
        name: module
    volumes:
    - name: module
      persistentVolumeClaim:
        claimName: workspace
    inputs:
      parameters:
      - {name: datetime}
    outputs:
      artifacts:
      - {name: crawling-data-data, path: /tmp/outputs/data/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.9
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--datetime", {"inputValue": "datetime"}, "--data", {"outputPath":
          "data"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''beautifulsoup4'' ''requests''
          ''fake_useragent'' ''pandas'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''beautifulsoup4'' ''requests''
          ''fake_useragent'' ''pandas'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef crawling_data(\n    datetime,\n    data_path,\n):  \n    import
          os\n    import sys\n    sys.path.append(''/module'')\n    from data_crawler
          import NaverNewsCrawler\n\n    cralwer = NaverNewsCrawler(date=datetime)\n    data
          = cralwer.get_news_data()\n    cralwer.create_news_data(data_path , data)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Crawling data'', description='''')\n_parser.add_argument(\"--datetime\",
          dest=\"datetime\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data\",
          dest=\"data_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = crawling_data(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs":
          [{"name": "datetime", "type": "String"}], "name": "Crawling data", "outputs":
          [{"name": "data", "type": "csv"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"datetime": "{{inputs.parameters.datetime}}"}'}
  - name: csv-to-json
    container:
      args: [--crawling-data, /tmp/inputs/crawling_data/data, --json-data, /tmp/outputs/json_data/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'dill' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas' 'dill' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def csv_to_json(
            crawling_data,
            json_data,
        ):
            import pandas as pd
            import json
            import dill

            df = pd.read_csv(crawling_data)
            df=df[['title','content', 'thumbnail','pcLinkUrl', 'officeName', 'updatedAt', 'createdAt']]

            js_data = json.loads(df.to_json(orient='records', force_ascii=False, date_format='iso'))

            with open(json_data, 'w', encoding='utf-8') as outfile:
                json.dump(js_data, outfile)

        import argparse
        _parser = argparse.ArgumentParser(prog='Csv to json', description='')
        _parser.add_argument("--crawling-data", dest="crawling_data", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--json-data", dest="json_data", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = csv_to_json(**_parsed_args)
      image: python:3.7
    inputs:
      artifacts:
      - {name: crawling-data-data, path: /tmp/inputs/crawling_data/data}
    outputs:
      artifacts:
      - {name: csv-to-json-json_data, path: /tmp/outputs/json_data/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.9
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--crawling-data", {"inputPath": "crawling_data"}, "--json-data",
          {"outputPath": "json_data"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' ''dill''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas'' ''dill'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef csv_to_json(\n    crawling_data,\n    json_data,\n):\n    import
          pandas as pd\n    import json\n    import dill\n\n    df = pd.read_csv(crawling_data)\n    df=df[[''title'',''content'',
          ''thumbnail'',''pcLinkUrl'', ''officeName'', ''updatedAt'', ''createdAt'']]\n\n    js_data
          = json.loads(df.to_json(orient=''records'', force_ascii=False, date_format=''iso''))\n\n    with
          open(json_data, ''w'', encoding=''utf-8'') as outfile:\n        json.dump(js_data,
          outfile)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Csv
          to json'', description='''')\n_parser.add_argument(\"--crawling-data\",
          dest=\"crawling_data\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--json-data\",
          dest=\"json_data\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = csv_to_json(**_parsed_args)\n"], "image": "python:3.7"}}, "inputs": [{"name":
          "crawling_data", "type": "csv"}], "name": "Csv to json", "outputs": [{"name":
          "json_data", "type": "json"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: es-data-update
    container:
      args: [--json-data, /tmp/inputs/json_data/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'elasticsearch==7.12.0' 'dill' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'elasticsearch==7.12.0' 'dill'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def es_data_update(\n    json_data,\n):\n    from elasticsearch import Elasticsearch\n\
        \    import json \n    import dill\n    import warnings\n    warnings.filterwarnings('ignore')\n\
        \n    # elasticsearch accesss\n    es = Elasticsearch(['https://elastic:4w88klz48xcgvdq8m9gsncvt@192.168.1.10:32311'],\
        \ verify_certs=False)\n\n    with open(json_data, encoding='utf-8') as json_file:\n\
        \        es_data = json.loads(json_file.read())\n\n    body = \"\"\n    for\
        \ i in es_data:\n        body = body + json.dumps({\"index\": {\"_index\"\
        : \"news_index\"}}) + '\\n'\n        body = body + json.dumps(i, ensure_ascii=False)\
        \ + \"\\n\"\n\n    es.bulk(body)    \n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Es\
        \ data update', description='')\n_parser.add_argument(\"--json-data\", dest=\"\
        json_data\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
        \ = vars(_parser.parse_args())\n\n_outputs = es_data_update(**_parsed_args)\n"
      image: python:3.7
    inputs:
      artifacts:
      - {name: csv-to-json-json_data, path: /tmp/inputs/json_data/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.9
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--json-data", {"inputPath": "json_data"}], "command": ["sh",
          "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''elasticsearch==7.12.0'' ''dill'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''elasticsearch==7.12.0''
          ''dill'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def es_data_update(\n    json_data,\n):\n    from elasticsearch import
          Elasticsearch\n    import json \n    import dill\n    import warnings\n    warnings.filterwarnings(''ignore'')\n\n    #
          elasticsearch accesss\n    es = Elasticsearch([''https://elastic:4w88klz48xcgvdq8m9gsncvt@192.168.1.10:32311''],
          verify_certs=False)\n\n    with open(json_data, encoding=''utf-8'') as json_file:\n        es_data
          = json.loads(json_file.read())\n\n    body = \"\"\n    for i in es_data:\n        body
          = body + json.dumps({\"index\": {\"_index\": \"news_index\"}}) + ''\\n''\n        body
          = body + json.dumps(i, ensure_ascii=False) + \"\\n\"\n\n    es.bulk(body)    \n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Es data update'', description='''')\n_parser.add_argument(\"--json-data\",
          dest=\"json_data\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = es_data_update(**_parsed_args)\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "json_data", "type": "json"}],
          "name": "Es data update"}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: news-data-crawling-pipeline
    inputs:
      parameters:
      - {name: datetime}
    dag:
      tasks:
      - name: crawling-data
        template: crawling-data
        arguments:
          parameters:
          - {name: datetime, value: '{{inputs.parameters.datetime}}'}
      - name: csv-to-json
        template: csv-to-json
        dependencies: [crawling-data]
        arguments:
          artifacts:
          - {name: crawling-data-data, from: '{{tasks.crawling-data.outputs.artifacts.crawling-data-data}}'}
      - name: es-data-update
        template: es-data-update
        dependencies: [csv-to-json]
        arguments:
          artifacts:
          - {name: csv-to-json-json_data, from: '{{tasks.csv-to-json.outputs.artifacts.csv-to-json-json_data}}'}
  arguments:
    parameters:
    - {name: datetime}
  serviceAccountName: pipeline-runner
